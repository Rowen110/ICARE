package com.icare.dataprocessing.service_examples;

import java.io.File;
import java.io.Serializable;
import java.util.Arrays;
import java.util.regex.Pattern;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.api.java.StorageLevels;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import com.ecams.claim.bo.ClaimHeader;
import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;


/**
 * Counts words in UTF8 encoded, '\n' delimited text received from the network every second.
 *
 * Usage: JavaNetworkWordCount <hostname> <port>
 * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *    `$ bin/run-example org.apache.spark.examples.streaming.JavaNetworkWordCount localhost 9999`
 */
public class IngestionService implements Serializable{
	  private static final Pattern SPACE = Pattern.compile(" ");
    
	  public static void main(String[] args) throws Exception {
	    if (args.length < 2) {
	      System.err.println("Usage: JavaNetworkWordCount <hostname> <port>");
	      System.exit(1);
	    }
	    
	    SparkConf conf = new SparkConf()
        .setMaster("local[*]")
        .setAppName("VerySimpleStreamingApp");
JavaStreamingContext ssc =
        new JavaStreamingContext(conf, Durations.seconds(5));
	    

	    // Create a JavaReceiverInputDStream on target ip:port and count the
	    // words in input stream of \n delimited text (eg. generated by 'nc')
	    // Note that no duplication in storage level only for running locally.
	    // Replication necessary in distributed scenario for fault tolerance.
	    JavaReceiverInputDStream<String> lines = ssc.socketTextStream(
	        "127.0.0.1", Integer.parseInt("9999"));
	    lines.print();
	    // Convert RDDs of the words DStream to DataFrame and run SQL query
	    lines.foreachRDD((rdd, time) -> {
	      SparkSession spark = JavaSparkSessionSingleton.getInstance(rdd.context().getConf());

	      // Convert JavaRDD[String] to JavaRDD[bean class] to DataFrame
	      JavaRDD<ClaimHeader> rowRDD = rdd.map(word -> {
	    	  System.out.println("WORD:"+ word);
	    	  ClaimHeader record = new ClaimHeader();
	    	 // ClaimHeader record =   objectMapper().readValue(word, ClaimHeader.class);
	        return record;
	      });
	      Dataset<Row> wordsDataFrame = spark.createDataFrame(rowRDD, ClaimHeader.class);
	      wordsDataFrame.show();
//	      wordsDataFrame.createOrReplaceTempView("words");
//
//	      Dataset<Row> wordCountsDataFrame =
//	          spark.sql("select word, count(*) as total from words group by word");
//	      System.out.println("========= " + time + "=========");
//	      wordCountsDataFrame.show();
	    });

	    ssc.start();
	    ssc.awaitTermination();
	  }
	  
	  public static ObjectMapper objectMapper(){
			ObjectMapper mapper = new ObjectMapper();
			mapper.configure(DeserializationFeature.UNWRAP_ROOT_VALUE, false);
			mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);
			mapper.enable(DeserializationFeature.ACCEPT_EMPTY_STRING_AS_NULL_OBJECT);
			mapper.enable(DeserializationFeature.ACCEPT_SINGLE_VALUE_AS_ARRAY);
		    return mapper;
		}
	  
	}

	/** Lazily instantiated singleton instance of SparkSession */
	class JavaSparkSessionSingleton {
	  private static transient SparkSession instance = null;
	  public static SparkSession getInstance(SparkConf sparkConf) {
	    if (instance == null) {
	      instance = SparkSession
	        .builder()
	        .config(sparkConf)
	        .getOrCreate();
	    }
	    return instance;
	  }
	}